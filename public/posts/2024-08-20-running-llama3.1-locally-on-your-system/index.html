<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process | Ram's Website</title>
<meta name=keywords content="AI,Machine Learning,LLaMA,Ollama"><meta name=description content="A comprehensive guide to running Meta‚Äôs LLaMA 3.1 locally using Ollama, covering installation, setup, and fine-tuning."><meta name=author content="Bhaskar"><link rel=canonical href=http://localhost:1313/posts/2024-08-20-running-llama3.1-locally-on-your-system/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2024-08-20-running-llama3.1-locally-on-your-system/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process"><meta property="og:description" content="A comprehensive guide to running Meta‚Äôs LLaMA 3.1 locally using Ollama, covering installation, setup, and fine-tuning."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2024-08-20-running-llama3.1-locally-on-your-system/"><meta property="og:image" content="https://bsmedia.business-standard.com/_media/bs/img/article/2024-04/15/full/1713166634-7252.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-20T00:00:00+00:00"><meta property="og:site_name" content="ram's website"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bsmedia.business-standard.com/_media/bs/img/article/2024-04/15/full/1713166634-7252.jpg"><meta name=twitter:title content="Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process"><meta name=twitter:description content="A comprehensive guide to running Meta‚Äôs LLaMA 3.1 locally using Ollama, covering installation, setup, and fine-tuning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process","item":"http://localhost:1313/posts/2024-08-20-running-llama3.1-locally-on-your-system/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process","name":"Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process","description":"A comprehensive guide to running Meta‚Äôs LLaMA 3.1 locally using Ollama, covering installation, setup, and fine-tuning.","keywords":["AI","Machine Learning","LLaMA","Ollama"],"articleBody":"Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process With the rapid advancement in AI and machine learning, large language models (LLMs) have become an integral part of various applications, from chatbots to content generation. Meta‚Äôs LLaMA 3.1 (Large Language Model Meta AI) is one of the most powerful models available, and running it locally allows developers and researchers to explore its capabilities without relying on cloud-based services. One of the easiest ways to set up and run LLaMA 3.1 locally is using Ollama, a platform designed to streamline the deployment of LLMs.\nIn this blog post, we‚Äôll walk you through the steps to get LLaMA 3.1 up and running on your local system using Ollama.\nWhy Run LLaMA 3.1 Locally? Running LLaMA 3.1 locally comes with several benefits:\nPrivacy: No data is sent to external servers, ensuring that your data remains confidential. Customization: You have full control over the model, allowing you to tweak and fine-tune it to suit your specific needs. Performance: Depending on your hardware, local execution can be faster and more reliable than cloud-based alternatives. Prerequisites Before you start, make sure you have the following:\nA capable machine: LLaMA 3.1 is a resource-intensive model, so you‚Äôll need a powerful machine, preferably with a GPU. At least 16GB of RAM and a high-end CPU are recommended. A dedicated GPU with a minimum of 8GB VRAM will significantly enhance performance. Python 3.8+ installed on your system. Docker: Ollama leverages Docker for containerized deployment, so ensure you have Docker installed and running on your system. NVIDIA Drivers and CUDA (for GPU acceleration): If you‚Äôre running the model on a GPU, make sure you have the appropriate NVIDIA drivers and CUDA toolkit installed. Step 1: Install Ollama Ollama is designed to simplify the process of running LLMs like LLaMA locally. To get started, you‚Äôll need to install Ollama.\nInstall Docker:\nOn Ubuntu: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io On macOS: Download and install Docker Desktop from here. On Windows: Download and install Docker Desktop from here. Install Ollama: Once Docker is installed, you can install Ollama by running:\ncurl -sSL https://ollama.com/install.sh | bash This script will download and set up Ollama on your system.\nStep 2: Set Up LLaMA 3.1 with Ollama After installing Ollama, you can now set up LLaMA 3.1.\nPull the LLaMA 3.1 Docker Image: Ollama provides pre-built Docker images for various models. To pull the LLaMA 3.1 image, run:\nollama pull llama:3.1 This command will download the LLaMA 3.1 model and its dependencies, which may take some time depending on your internet connection.\nVerify the Installation: After the image is downloaded, you can verify that everything is set up correctly by running:\nollama list This command should display LLaMA 3.1 as one of the available models.\nStep 3: Running LLaMA 3.1 Locally With everything set up, you can now run LLaMA 3.1 locally.\nStart the LLaMA 3.1 Container: To start the LLaMA 3.1 container, use the following command:\nollama run llama:3.1 This will launch the LLaMA 3.1 model inside a Docker container, ready to accept queries.\nInteracting with the Model: You can interact with LLaMA 3.1 by sending input queries. For example:\nollama query llama:3.1 \"What is the capital of France?\" The model will process the query and return a response, such as ‚ÄúParis is the capital of France.‚Äù\nStep 4: Fine-Tuning (Optional) One of the advantages of running LLaMA 3.1 locally is the ability to fine-tune the model. Fine-tuning involves adjusting the model parameters based on your dataset, making the model more accurate for specific tasks.\nTo fine-tune LLaMA 3.1, you‚Äôll need a labeled dataset and a configuration file. The process involves several steps:\nPrepare your dataset in a format compatible with Ollama. Create a configuration file specifying the training parameters. Run the fine-tuning command: ollama train llama:3.1 --config your_config_file.yaml --data your_dataset Conclusion Running LLaMA 3.1 locally using Ollama is a powerful way to harness the capabilities of this advanced language model. Whether you‚Äôre a researcher, developer, or enthusiast, this setup allows you to experiment with state-of-the-art AI without relying on cloud-based services.\nBy following the steps outlined in this guide, you can get LLaMA 3.1 up and running on your system in no time. From there, the possibilities are endless‚Äîfrom natural language processing tasks to AI-driven content creation. So, get started today and explore the world of large language models on your own terms!\n","wordCount":"733","inLanguage":"en","image":"https://bsmedia.business-standard.com/_media/bs/img/article/2024-04/15/full/1713166634-7252.jpg","datePublished":"2024-08-20T00:00:00Z","dateModified":"2024-08-20T00:00:00Z","author":{"@type":"Person","name":"Bhaskar"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2024-08-20-running-llama3.1-locally-on-your-system/"},"publisher":{"@type":"Organization","name":"Ram's Website","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Ô£ø root@bhaskarvilles ‚öï (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Ô£ø root@bhaskarvilles ‚öï</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title="üöÄ categories"><span>üöÄ categories</span></a></li><li><a href=http://localhost:1313/tags/ title="üí• tags"><span>üí• tags</span></a></li><li><a href=https://kerdos.io title=company><span>company</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process</h1><div class=post-description>A comprehensive guide to running Meta‚Äôs LLaMA 3.1 locally using Ollama, covering installation, setup, and fine-tuning.</div><div class=post-meta><span title='2024-08-20 00:00:00 +0000 UTC'>August 20, 2024</span>&nbsp;¬∑&nbsp;4 min&nbsp;¬∑&nbsp;733 words&nbsp;¬∑&nbsp;Bhaskar&nbsp;|&nbsp;<a href=https://github.com/bhaskarvilles/content/posts/2024-08-20-running-llama3.1-locally-on-your-system.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h3 id=running-llama-31-locally-with-ollama-a-step-by-step-process>Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process<a hidden class=anchor aria-hidden=true href=#running-llama-31-locally-with-ollama-a-step-by-step-process>#</a></h3><p>With the rapid advancement in AI and machine learning, large language models (LLMs) have become an integral part of various applications, from chatbots to content generation. Meta‚Äôs LLaMA 3.1 (Large Language Model Meta AI) is one of the most powerful models available, and running it locally allows developers and researchers to explore its capabilities without relying on cloud-based services. One of the easiest ways to set up and run LLaMA 3.1 locally is using Ollama, a platform designed to streamline the deployment of LLMs.</p><p>In this blog post, we‚Äôll walk you through the steps to get LLaMA 3.1 up and running on your local system using Ollama.</p><hr><h3 id=why-run-llama-31-locally><strong>Why Run LLaMA 3.1 Locally?</strong><a hidden class=anchor aria-hidden=true href=#why-run-llama-31-locally>#</a></h3><p>Running LLaMA 3.1 locally comes with several benefits:</p><ul><li><strong>Privacy:</strong> No data is sent to external servers, ensuring that your data remains confidential.</li><li><strong>Customization:</strong> You have full control over the model, allowing you to tweak and fine-tune it to suit your specific needs.</li><li><strong>Performance:</strong> Depending on your hardware, local execution can be faster and more reliable than cloud-based alternatives.</li></ul><p><img loading=lazy src=https://about.fb.com/wp-content/uploads/2024/04/Meta-AI-Expasion_Header.gif alt="Meta Llama3.1"></p><h3 id=prerequisites><strong>Prerequisites</strong><a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h3><p>Before you start, make sure you have the following:</p><ul><li><strong>A capable machine:</strong> LLaMA 3.1 is a resource-intensive model, so you&rsquo;ll need a powerful machine, preferably with a GPU. At least 16GB of RAM and a high-end CPU are recommended. A dedicated GPU with a minimum of 8GB VRAM will significantly enhance performance.</li><li><strong>Python 3.8+ installed</strong> on your system.</li><li><strong>Docker:</strong> Ollama leverages Docker for containerized deployment, so ensure you have Docker installed and running on your system.</li><li><strong>NVIDIA Drivers and CUDA (for GPU acceleration):</strong> If you&rsquo;re running the model on a GPU, make sure you have the appropriate NVIDIA drivers and CUDA toolkit installed.</li></ul><h3 id=step-1-install-ollama><strong>Step 1: Install Ollama</strong><a hidden class=anchor aria-hidden=true href=#step-1-install-ollama>#</a></h3><p>Ollama is designed to simplify the process of running LLMs like LLaMA locally. To get started, you&rsquo;ll need to install Ollama.</p><ol><li><p><strong>Install Docker:</strong></p><ul><li>On Ubuntu:<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt-get update
</span></span><span class=line><span class=cl>sudo apt-get install docker-ce docker-ce-cli containerd.io
</span></span></code></pre></div></li><li>On macOS:<ul><li>Download and install Docker Desktop from <a href=https://www.docker.com/products/docker-desktop>here</a>.</li></ul></li><li>On Windows:<ul><li>Download and install Docker Desktop from <a href=https://www.docker.com/products/docker-desktop>here</a>.</li></ul></li></ul></li><li><p><strong>Install Ollama:</strong>
Once Docker is installed, you can install Ollama by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl -sSL https://ollama.com/install.sh <span class=p>|</span> bash
</span></span></code></pre></div><p>This script will download and set up Ollama on your system.</p></li></ol><h3 id=step-2-set-up-llama-31-with-ollama><strong>Step 2: Set Up LLaMA 3.1 with Ollama</strong><a hidden class=anchor aria-hidden=true href=#step-2-set-up-llama-31-with-ollama>#</a></h3><p>After installing Ollama, you can now set up LLaMA 3.1.</p><ol><li><p><strong>Pull the LLaMA 3.1 Docker Image:</strong>
Ollama provides pre-built Docker images for various models. To pull the LLaMA 3.1 image, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama pull llama:3.1
</span></span></code></pre></div><p>This command will download the LLaMA 3.1 model and its dependencies, which may take some time depending on your internet connection.</p></li><li><p><strong>Verify the Installation:</strong>
After the image is downloaded, you can verify that everything is set up correctly by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama list
</span></span></code></pre></div><p>This command should display LLaMA 3.1 as one of the available models.</p></li></ol><h3 id=step-3-running-llama-31-locally><strong>Step 3: Running LLaMA 3.1 Locally</strong><a hidden class=anchor aria-hidden=true href=#step-3-running-llama-31-locally>#</a></h3><p>With everything set up, you can now run LLaMA 3.1 locally.</p><ol><li><p><strong>Start the LLaMA 3.1 Container:</strong>
To start the LLaMA 3.1 container, use the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama run llama:3.1
</span></span></code></pre></div><p>This will launch the LLaMA 3.1 model inside a Docker container, ready to accept queries.</p></li><li><p><strong>Interacting with the Model:</strong>
You can interact with LLaMA 3.1 by sending input queries. For example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama query llama:3.1 <span class=s2>&#34;What is the capital of France?&#34;</span>
</span></span></code></pre></div><p>The model will process the query and return a response, such as &ldquo;Paris is the capital of France.&rdquo;</p></li></ol><p><img loading=lazy src=https://about.fb.com/wp-content/uploads/2024/04/04_Seamless-Search-1.gif alt="Meta Ai Released by Meta.ai"></p><h3 id=step-4-fine-tuning-optional><strong>Step 4: Fine-Tuning (Optional)</strong><a hidden class=anchor aria-hidden=true href=#step-4-fine-tuning-optional>#</a></h3><p>One of the advantages of running LLaMA 3.1 locally is the ability to fine-tune the model. Fine-tuning involves adjusting the model parameters based on your dataset, making the model more accurate for specific tasks.</p><p>To fine-tune LLaMA 3.1, you&rsquo;ll need a labeled dataset and a configuration file. The process involves several steps:</p><ol><li><strong>Prepare your dataset</strong> in a format compatible with Ollama.</li><li><strong>Create a configuration file</strong> specifying the training parameters.</li><li><strong>Run the fine-tuning command</strong>:<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama train llama:3.1 --config your_config_file.yaml --data your_dataset
</span></span></code></pre></div></li></ol><h3 id=conclusion><strong>Conclusion</strong><a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>Running LLaMA 3.1 locally using Ollama is a powerful way to harness the capabilities of this advanced language model. Whether you&rsquo;re a researcher, developer, or enthusiast, this setup allows you to experiment with state-of-the-art AI without relying on cloud-based services.</p><p>By following the steps outlined in this guide, you can get LLaMA 3.1 up and running on your system in no time. From there, the possibilities are endless‚Äîfrom natural language processing tasks to AI-driven content creation. So, get started today and explore the world of large language models on your own terms!</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ai/>AI</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1313/tags/llama/>LLaMA</a></li><li><a href=http://localhost:1313/tags/ollama/>Ollama</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/2024-08-21-arm-processors/><span class=title>¬´ Prev</span><br><span>Advanced RISC Machines</span>
</a><a class=next href=http://localhost:1313/posts/2024-08-18-evacuation-in-ukrain/><span class=title>Next ¬ª</span><br><span>Evacuation Urged in Ukrainian-Controlled DPR Cities: A Dire Call for Safety</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process on x" href="https://x.com/intent/tweet/?text=Running%20LLaMA%203.1%20Locally%20with%20Ollama%3a%20A%20Step-by-Step%20process&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-08-20-running-llama3.1-locally-on-your-system%2f&amp;hashtags=AI%2cMachineLearning%2cLLaMA%2cOllama"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-08-20-running-llama3.1-locally-on-your-system%2f&amp;title=Running%20LLaMA%203.1%20Locally%20with%20Ollama%3a%20A%20Step-by-Step%20process&amp;summary=Running%20LLaMA%203.1%20Locally%20with%20Ollama%3a%20A%20Step-by-Step%20process&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-08-20-running-llama3.1-locally-on-your-system%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-08-20-running-llama3.1-locally-on-your-system%2f&title=Running%20LLaMA%203.1%20Locally%20with%20Ollama%3a%20A%20Step-by-Step%20process"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-08-20-running-llama3.1-locally-on-your-system%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process on whatsapp" href="https://api.whatsapp.com/send?text=Running%20LLaMA%203.1%20Locally%20with%20Ollama%3a%20A%20Step-by-Step%20process%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-08-20-running-llama3.1-locally-on-your-system%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process on telegram" href="https://telegram.me/share/url?text=Running%20LLaMA%203.1%20Locally%20with%20Ollama%3a%20A%20Step-by-Step%20process&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-08-20-running-llama3.1-locally-on-your-system%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running LLaMA 3.1 Locally with Ollama: A Step-by-Step process on ycombinator" href="https://news.ycombinator.com/submitlink?t=Running%20LLaMA%203.1%20Locally%20with%20Ollama%3a%20A%20Step-by-Step%20process&u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-08-20-running-llama3.1-locally-on-your-system%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Ram's Website</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>